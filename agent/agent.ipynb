{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Yanzheng Wu(yanzheng.wu@tufts.edu)\n",
    "Reference: NandaKishore Joshi - https://github.com/NandaKishoreJoshi/Reinforcement_Lerning\n",
    "\"\"\"\n",
    "import Gridworld as GW\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pylab as plt\n",
    "import random\n",
    "\n",
    "game = GW.Gridworld(size=4, mode=\"static\")\n",
    "\n",
    "l1 = 64\n",
    "l2 = 150\n",
    "l3 = 100\n",
    "l4 = 4\n",
    "gamma = 0.9\n",
    "epsilon = 1.0\n",
    "\n",
    "#NN model for the agent\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(l1, l2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(l2, l3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(l3,l4)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "losses = []   \n",
    "action_set = {\n",
    "    0: 'u',\n",
    "    1: 'd',\n",
    "    2: 'l',\n",
    "    3: 'r',\n",
    "}\n",
    "for i in range(epochs):                                \n",
    "    game = GW.Gridworld(size=4, mode='static')            \n",
    "    state_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0               \n",
    "    state1 = torch.from_numpy(state_).float()          \n",
    "    status = 1                                         \n",
    "    while(status == 1):                                \n",
    "        qval = model(state1)                           \n",
    "        qval_ = qval.data.numpy()\n",
    "        if (random.random() < epsilon):                \n",
    "            action_ = np.random.randint(0,4)\n",
    "        else:\n",
    "            action_ = np.argmax(qval_)\n",
    "        \n",
    "        action = action_set[action_]                   \n",
    "        game.makeMove(action)                          \n",
    "        state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "        state2 = torch.from_numpy(state2_).float()     \n",
    "        reward = game.reward()\n",
    "        with torch.no_grad():\n",
    "            newQ = model(state2.reshape(1,64))\n",
    "        maxQ = torch.max(newQ)                         \n",
    "        if reward == -1:                               \n",
    "            Y = reward + (gamma * maxQ)\n",
    "        else:\n",
    "            Y = reward\n",
    "        Y = torch.Tensor([Y]).detach()\n",
    "        X = qval.squeeze()[action_]                    \n",
    "        loss = loss_fn(X, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        state1 = state2\n",
    "        if reward != -1:                               \n",
    "            status = 0\n",
    "    if epsilon > 0.1:                                  \n",
    "        epsilon -= (1/epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('rl38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af41bf0432429ffb1fcafe29cd469ffe6d4dc458b55e180dade11426f674c9b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
