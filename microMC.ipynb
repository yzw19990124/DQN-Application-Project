{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "microMC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsnaRPOZMl5x"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "#from chainer import cuda\n",
        "\n",
        "#import cupy as cp\n",
        "\n",
        "#backend\n",
        "#be = \"gpu\"\n",
        "#device = 0\n",
        "\n",
        "\n",
        "be = \"cpu\"\n",
        "\n",
        "class SimplePG(object):\n",
        "\t\n",
        "\t\n",
        "\t# constructor\n",
        "\tdef __init__(self, num_actions, input_size, hidden_layer_size, learning_rate,gamma,decay_rate,greedy_e_epsilon,random_seed):\n",
        "\t\t# store hyper-params\n",
        "\t\tself._A = num_actions\n",
        "\t\tself._D = input_size\n",
        "\t\tself._H = hidden_layer_size\n",
        "\t\tself._learning_rate = learning_rate\n",
        "\t\tself._decay_rate = decay_rate\n",
        "\t\tself._gamma = gamma\n",
        "\t\t\n",
        "\t\t# some temp variables\n",
        "\t\tself._xs,self._hs,self._dlogps,self._drs = [],[],[],[]\n",
        "\n",
        "\t\t# variables governing exploration\n",
        "\t\tself._exploration = True # should be set to false when evaluating\n",
        "\t\tself._explore_eps = greedy_e_epsilon\n",
        "\t\t\n",
        "\t\t#create model\n",
        "\t\tself.init_model(random_seed)\n",
        "\t\t\n",
        "\t\n",
        "\tdef init_model(self,random_seed):\n",
        "\t\t# create model\n",
        "\t\t#with cp.cuda.Device(0):\n",
        "\t\tself._model = {}\n",
        "\t\tnp.random.seed(random_seed)\n",
        "\t   \n",
        "\t\t# weights from input to hidden layer   \n",
        "\t\tself._model['W1'] = np.random.randn(self._D,self._H) / np.sqrt(self._D) # \"Xavier\" initialization\n",
        "\t   \n",
        "\t\t# weights from hidden to output (action) layer\n",
        "\t\tself._model['W2'] = np.random.randn(self._H,self._A) / np.sqrt(self._H)\n",
        "\t\t\t\n",
        "\t\t\t\t\n",
        "\t\tself._grad_buffer = { k : np.zeros_like(v) for k,v in self._model.items() } # update buffers that add up gradients over a batch\n",
        "\t\tself._rmsprop_cache = { k : np.zeros_like(v) for k,v in self._model.items() } # rmsprop memory\n",
        "\n",
        "\t\n",
        "\t# softmax function\n",
        "\tdef softmax(self,x):\n",
        "\t\tprobs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "\t\tprobs /= np.sum(probs, axis=1, keepdims=True)\n",
        "\t\treturn probs\n",
        "\t\t\n",
        "\t  \n",
        "\tdef discount_rewards(self,r):\n",
        "\t\t\"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "\t\tdiscounted_r = np.zeros_like(r)\n",
        "\t\trunning_add = 0\n",
        "\t\tfor t in reversed(range(0, r.size)):\n",
        "\t\t\trunning_add = running_add * self._gamma + r[t]\n",
        "\t\t\tdiscounted_r[t] = float(running_add)\n",
        "    \n",
        "\t\treturn discounted_r\n",
        "\t\n",
        "\t# feed input to network and get result\n",
        "\tdef policy_forward(self,x):\n",
        "\t\tif(len(x.shape)==1):\n",
        "\t\t\tx = x[np.newaxis,...]\n",
        "\n",
        "\t\th = x.dot(self._model['W1'])\n",
        "\t\t\n",
        "\t\tif np.isnan(np.sum(self._model['W1'])):\n",
        "\t\t\tprint(\"W1 sum is nan\")\n",
        "\t\t\n",
        "\t\tif np.isnan(np.sum(self._model['W2'])):\n",
        "\t\t\tprint(\"W2 sum is nan\")\n",
        "\t\t\n",
        "\t\tif np.isnan(np.sum(h)):\n",
        "\t\t\tprint(\"nan\")\n",
        "\t\t\t\n",
        "\t\t\th[np.isnan(h)] = np.random.random_sample()\n",
        "\t\t\th[np.isinf(h)] = np.random.random_sample()\n",
        "\t\t\t\n",
        "\n",
        "\t\tif np.isnan(np.sum(h)):\n",
        "\t\t\tprint(\"Still nan!\")\n",
        "\t\t\n",
        "\t\t\n",
        "\t\th[h<0] = 0 # ReLU nonlinearity\n",
        "\t\tlogp = h.dot(self._model['W2'])\n",
        "\n",
        "\t\tp = self.softmax(logp)\n",
        "  \n",
        "\t\treturn p, h # return probability of taking actions, and hidden state\n",
        "\t\t\n",
        "\t\n",
        "\tdef policy_backward(self,eph, epdlogp):\n",
        "\t\t\"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "\t\tdW2 = eph.T.dot(epdlogp)  \n",
        "\t\tdh = epdlogp.dot(self._model['W2'].T)\n",
        "\t\tdh[eph <= 0] = 0 # backpro prelu\n",
        "  \n",
        "\t\tt = time.time()\n",
        "  \n",
        "\t\tif(be == \"gpu\"):\n",
        "\t\t  self._dh_gpu = cuda.to_gpu(dh, device=0)\n",
        "\t\t  self._epx_gpu = cuda.to_gpu(self._epx.T, device=0)\n",
        "\t\t  self._dW1 = cuda.to_cpu(self._epx_gpu.dot(self._dh_gpu) )\n",
        "\t\telse:\n",
        "\t\t  self._dW1 = self._epx.T.dot(dh) \n",
        "    \n",
        "\n",
        "\t\t#print((time.time()-t0)*1000, ' ms, @final bprop')\n",
        "\n",
        "\t\treturn {'W1':self._dW1, 'W2':dW2}\n",
        "\t\n",
        "\tdef set_explore_epsilon(self,e):\n",
        "\t\tself._explore_eps = e\n",
        "\t\n",
        "\t# input: current state/observation\n",
        "\t# output: action index\n",
        "\tdef process_step(self, x, exploring):\n",
        "\n",
        "\t\t# feed input through network and get output action distribution and hidden layer\n",
        "\t\taprob, h = self.policy_forward(x)\n",
        "\t\t\n",
        "\t\t#print(aprob)\n",
        "\t\t\n",
        "\t\t# if exploring\n",
        "\t\tif exploring == True:\n",
        "\t\t\t\n",
        "\t\t\t# greedy-e exploration\n",
        "\t\t\trand_e = np.random.uniform()\n",
        "\t\t\t#print(rand_e)\n",
        "\t\t\tif rand_e < self._explore_eps:\n",
        "\t\t\t\t# set all actions to be equal probability\n",
        "\t\t\t\taprob[0] = [ 1.0/len(aprob[0]) for i in range(len(aprob[0]))]\n",
        "\t\t\t\t#print(\"!\")\n",
        "\t\t\n",
        "\t\t\n",
        "\t\tif np.isnan(np.sum(aprob)):\n",
        "\t\t\tprint(aprob)\n",
        "\t\t\taprob[0] = [ 1.0/len(aprob[0]) for i in range(len(aprob[0]))]\n",
        "\t\t\tprint(aprob)\n",
        "\t\t\t#input()\n",
        "\t\t\n",
        "\t\taprob_cum = np.cumsum(aprob)\n",
        "\t\tu = np.random.uniform()\n",
        "\t\ta = np.where(u <= aprob_cum)[0][0]\t\n",
        "\n",
        "\t\t# record various intermediates (needed later for backprop)\n",
        "\t\tt = time.time()\n",
        "\t\tself._xs.append(x) # observation\n",
        "\t\tself._hs.append(h)\n",
        "\n",
        "\t\t#softmax loss gradient\n",
        "\t\tdlogsoftmax = aprob.copy()\n",
        "\t\tdlogsoftmax[0,a] -= 1 #-discounted reward \n",
        "\t\tself._dlogps.append(dlogsoftmax)\n",
        "\t\t\n",
        "\t\tt  = time.time()\n",
        "\n",
        "\t\treturn a\n",
        "\t\t\n",
        "\t# after process_step, this function needs to be called to set the reward\n",
        "\tdef give_reward(self,reward):\n",
        "\t\t\n",
        "\t\t# store the reward in the list of rewards\n",
        "\t\tself._drs.append(reward)\n",
        "\t\t\n",
        "\t# reset to be used when evaluating\n",
        "\tdef reset(self):\n",
        "\t\tself._xs,self._hs,self._dlogps,self._drs = [],[],[],[] # reset \n",
        "\t\tself._grad_buffer = { k : np.zeros_like(v) for k,v in self._model.items() } # update buffers that add up gradients over a batch\n",
        "\t\tself._rmsprop_cache = { k : np.zeros_like(v) for k,v in self._model.items() } # rmsprop memory\n",
        "\n",
        "\t\t\n",
        "\t# this function should be called when an episode (i.e., a game) has finished\n",
        "\tdef finish_episode(self):\n",
        "\t\t# stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "\t\t\n",
        "\t\t# this needs to be stored to be used by policy_backward\n",
        "\t\t# self._xs is a list of vectors of size input dim and the number of vectors is equal to the number of time steps in the episode\n",
        "\t\tself._epx = np.vstack(self._xs)\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t#for i in range(0,len(self._hs)):\n",
        "\t\t#\tprint(self._hs[i])\n",
        "\t\t\n",
        "\t\t# len(self._hs) = # time steps\n",
        "\t\t# stores hidden state activations\n",
        "\t\teph = np.vstack(self._hs)\n",
        "\t\t\n",
        "\t\t#for i in range(0,len(self._dlogps)):\n",
        "\t\t#\tprint(self._dlogps[i])\n",
        "\t\t\n",
        "\t\t# self._dlogps stores a history of the probabilities over actions selected by the agent\n",
        "\t\tepdlogp = np.vstack(self._dlogps)\n",
        "\t\t\n",
        "\t\t# self._drs is the history of rewards\n",
        "\t\t#for i in range(0,len(self._drs)):\n",
        "\t\t#\tprint(self._drs[i])\n",
        "\t\tepr = np.vstack(self._drs)\n",
        "\t\t\n",
        "\t\tself._xs,self._hs,self._dlogps,self._drs = [],[],[],[] # reset array memory\n",
        "\n",
        "\t\t# compute the discounted reward backwards through time\n",
        "\t\tdiscounted_epr = (self.discount_rewards(epr))\n",
        "\t\t#for i in range(0,len(discounted_epr)):\n",
        "\t\t#\tprint(str(discounted_epr[i]) + \"\\t\"+str(epr[i]))\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t#print(discounted_epr)\n",
        "\t\tdiscounted_epr_mean = np.mean(discounted_epr)\n",
        "\t\t#print(discounted_epr_mean)\n",
        "\t\t\n",
        "\t\t# standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "\t\t\n",
        "\t\t#discounted_epr -= np.mean(discounted_epr)\n",
        "\t\tdiscounted_epr = np.subtract(discounted_epr,discounted_epr_mean)\n",
        "\t\t\n",
        "\t\t\n",
        "\t\tdiscounted_epr /= np.std(discounted_epr)\n",
        "\t\t\n",
        "\t\tepdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "\t\t\n",
        "\t\tstart_time = time.time()\n",
        "\t\tgrad = self.policy_backward(eph, epdlogp)\n",
        "\t\t#print(\"--- %s seconds for policy backward ---\" % (time.time() - start_time))\n",
        "\t\t\n",
        "\t\tfor k in self._model: self._grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "\t# called to update model parameters, generally every N episodes/games for some N\n",
        "\tdef update_parameters(self):\n",
        "\t\tfor k,v in self._model.items():\n",
        "\t\t\tg = self._grad_buffer[k] # gradient\n",
        "\t\t\tself._rmsprop_cache[k] = self._decay_rate * self._rmsprop_cache[k] + (1 - self._decay_rate) * g**2\n",
        "\t\t\tself._model[k] -= self._learning_rate * g / (np.sqrt(self._rmsprop_cache[k]) + 1e-5)\n",
        "\t\t\tself._grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "        \n",
        "        \n",
        "\t\t\n",
        "\t\t\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-H4xAPQM0GH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9036ec0-e222-4bc6-8300-b488714af40a"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import our dqn agent\n",
        "\n",
        "REWARD_STEP = -1\n",
        "REWARD_DONE = 1000\n",
        "\n",
        "\"\"\"\n",
        "grid key:\n",
        "0:\tempty space\n",
        "1: \twall\n",
        "2:\ttree\n",
        "3:\trock\n",
        "4:\tcraft table\n",
        "\n",
        "action keys:\n",
        "W:\tgo forward\n",
        "A:\tturn left\n",
        "D:\tturn right\n",
        "U:\tuse/break block (must be facing tree or rock)\n",
        "C: \tcraft (must be facing craft table and have at least 2 wood and 1 stone)\n",
        "\n",
        "use action key:\n",
        "tree -> add 1 wood\n",
        "rock -> add 1 stone\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class LidarSensor(object):\n",
        "\tdef __init__(self,increment):\n",
        "\t\t\n",
        "\t\tself.angle_increment = increment # e.g., 2pi/10 if we want 10 beams\n",
        "\t\tself.sense_range = 10\n",
        "\t\t\n",
        "\t\t\n",
        "\tdef sense(self,mc_env):\n",
        "\t\tnum_obj_types = len(mc_env.object_types) # this includes empty square\n",
        "\t\t\n",
        "\t\tturn_offset = 0\n",
        "\t\t\n",
        "\t\tif mc_env.agent_dir == 'W':\n",
        "\t\t\tturn_offset -= np.pi/2\n",
        "\t\telif mc_env.agent_dir == 'S':\n",
        "\t\t\tturn_offset -= 2*np.pi/2\n",
        "\t\telif mc_env.agent_dir == 'E':\n",
        "\t\t\tturn_offset -= 3*np.pi/2\n",
        "\t\t\n",
        "\t\tcurrent_angle = 0 + turn_offset\n",
        "\t\t\n",
        "\t\tlidar_readings = []\n",
        "\t\t\n",
        "\t\t#print(\"Agent location: \"+str(mc_env.agent_x)+\",\"+str(mc_env.agent_y))\n",
        "\t\t\n",
        "\t\twhile True:\n",
        "\t\t\t#print(\"Beam at angle = \"+str(current_angle))\n",
        "\t\t\t\n",
        "\t\t\tbeam_i = np.zeros(num_obj_types) # we get 1 value per object, excluding empty square\n",
        "\n",
        "\t\t\t# shoot beam\n",
        "\t\t\tfor r in range(1,self.sense_range):\n",
        "\t\t\t\t# calc x and y position of beam at length r relative to agent\n",
        "\t\t\t\tx = mc_env.agent_x + np.round(r*np.cos(current_angle))\n",
        "\t\t\t\ty = mc_env.agent_y + np.round(r*np.sin(current_angle))\n",
        "\t\t\t\tobj_xy = mc_env.grid[int(x)][int(y)]\n",
        "\t\t\t\t\n",
        "\t\t\t\tif not obj_xy == 0: # if square is not empty\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsensor_value = float(self.sense_range - r)/float(self.sense_range)\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t#print(str(int(x))+\",\"+str(int(y))+\",\"+str(int(obj_xy))+\",\"+str(sensor_value))\n",
        "\t\t\t\t\tbeam_i[obj_xy-1]=sensor_value\n",
        "\t\t\t\t\t#print beam_i\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\n",
        "\t\t\tfor k in range(0,len(beam_i)):\n",
        "\t\t\t\tlidar_readings.append(beam_i[k])\n",
        "\t\t\t#print lidar_readings\t\n",
        "\t\t\t\n",
        "\t\t\tcurrent_angle += self.angle_increment\n",
        "\t\t\t\n",
        "\t\t\tif current_angle >= 2*np.pi + turn_offset:\n",
        "\t\t\t\tbreak\n",
        "\t\treturn lidar_readings\n",
        "\n",
        "\n",
        "class microMC(object):\n",
        "\t\n",
        "\t# constructor\n",
        "\tdef __init__(self, width, height,random_seed):\n",
        "\t\t\n",
        "\t\t#np.random.seed(random_seed)\n",
        "\t\t\n",
        "\t\tself.object_types = [1, 2, 3, 4] # we have 4 objects: wall, tree, rock, and craft table\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\tself.reset(width,height,random_seed)\n",
        "\t\t\n",
        "\tdef reset(self, width, height,random_seed):\n",
        "\t\trows, cols = (width, height) \n",
        "\t\t\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.grid = [[0 for i in range(cols)] for j in range(rows)] \n",
        "\t\t\n",
        "\t\t# how many trees and rocks\n",
        "\t\tn_trees = width*height/20\n",
        "\t\tn_rocks = width*height/40\n",
        "\t\t\n",
        "\t\t# fill in walls with 1s\n",
        "\t\tfor i in range(0,self.height):\n",
        "\t\t\tfor j in range(0,self.width):\n",
        "\t\t\t\tif i == 0 or i == self.height-1 or j == 0 or j == self.width-1:\n",
        "\t\t\t\t\tself.grid[j][i] = 1\n",
        "\t\t\t\t\t\n",
        "\t\t# create random trees\n",
        "\t\tfor k in range(0,int(n_trees)):\n",
        "\t\t\tx_k = np.random.randint(self.width-2)+1\n",
        "\t\t\ty_k = np.random.randint(self.height-2)+1\n",
        "\t\t\tself.grid[x_k][y_k]=2\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\t# create random rocks\n",
        "\t\tfor k in range(0,int(n_rocks)):\n",
        "\t\t\tx_k = np.random.randint(self.width-2)+1\n",
        "\t\t\ty_k = np.random.randint(self.height-2)+1\n",
        "\t\t\tself.grid[x_k][y_k]=3\n",
        "\t\t\t\n",
        "\t\t# create crafting table\n",
        "\t\twhile True:\n",
        "\t\t\tx_k = np.random.randint(self.width-4)+2\n",
        "\t\t\ty_k = np.random.randint(self.height-4)+2\n",
        "\t\t\tif self.grid[x_k][y_k] == 0:\n",
        "\t\t\t\tself.grid[x_k][y_k] = 4\n",
        "\t\t\t\tbreak\n",
        "\t\t\n",
        "\t\t# initialize agent position and inventory\n",
        "\t\twhile True:\n",
        "\t\t\tx_k = np.random.randint(self.width-4)+2\n",
        "\t\t\ty_k = np.random.randint(self.height-4)+2\n",
        "\t\t\tif self.grid[x_k][y_k] == 0:\n",
        "\t\t\t\tself.agent_x = x_k\n",
        "\t\t\t\tself.agent_y = y_k\n",
        "\t\t\t\tself.agent_dir = 'N' # start facing north\n",
        "\t\t\t\tbreak\n",
        "\t\t\n",
        "\t\tself.inventory = dict([('wood', 0), ('stone', 0),('pogo',0)])\n",
        "\t\t\n",
        "\tdef toString(self):\n",
        "\t\tout_str = ''\n",
        "\t\tfor i in range(0,self.height):\n",
        "\t\t\tfor j in range(0,self.width):\n",
        "\t\t\t\tif self.agent_x == j and self.agent_y == i:\n",
        "\t\t\t\t\tif self.agent_dir == 'N':\n",
        "\t\t\t\t\t\tout_str += '^'\n",
        "\t\t\t\t\telif self.agent_dir == 'S':\n",
        "\t\t\t\t\t\tout_str += 'v'\n",
        "\t\t\t\t\telif self.agent_dir == 'E':\n",
        "\t\t\t\t\t\tout_str += '>'\n",
        "\t\t\t\t\telif self.agent_dir == 'W':\n",
        "\t\t\t\t\t\tout_str += '<'\n",
        "\t\t\t\telif self.grid[j][i] == 1: # obstacle/wall\n",
        "\t\t\t\t\tout_str += '#'\n",
        "\t\t\t\telif self.grid[j][i] == 2: # tree\n",
        "\t\t\t\t\tout_str += 'T'\n",
        "\t\t\t\telif self.grid[j][i] == 3: # rocks\n",
        "\t\t\t\t\tout_str += 'R'\n",
        "\t\t\t\telif self.grid[j][i] == 4: # craft table\n",
        "\t\t\t\t\tout_str += 'C'\n",
        "\t\t\t\telif self.grid[j][i] == 0: # free space\n",
        "\t\t\t\t\tout_str += ' '\n",
        "\t\t\t\t\n",
        "\t\t\t\t#out_str += str(self.grid[j][i])\n",
        "\t\t\t\tout_str += ' '\n",
        "\t\t\tout_str += '\\n'\n",
        "\t\t\n",
        "\t\tout_str += '\\ninventory:\\t' + str(self.inventory)\n",
        "\t\t\n",
        "\t\treturn out_str\n",
        "\t\n",
        "\tdef getFacingXY(self): # get the x y position in front of the agent\n",
        "\t\t# compute the target position in front of the agent\n",
        "\t\ttarget_x = self.agent_x\n",
        "\t\ttarget_y = self.agent_y\n",
        "\t\t\n",
        "\t\tif self.agent_dir == 'N':\n",
        "\t\t\ttarget_y -= 1\n",
        "\t\telif self.agent_dir == 'W':\n",
        "\t\t\ttarget_x -= 1\n",
        "\t\telif self.agent_dir == 'E':\n",
        "\t\t\ttarget_x += 1\n",
        "\t\telif self.agent_dir == 'S':\n",
        "\t\t\ttarget_y += 1\n",
        "\t\treturn [target_x,target_y]\n",
        "\t\n",
        "\tdef execute_action(self, action):\n",
        "\t\treward = REWARD_STEP\n",
        "\t\tdone = False\n",
        "\t\t\n",
        "\t\t# first, process turn actions\n",
        "\t\tif action == 'A': # turn right\n",
        "\t\t\tif self.agent_dir == 'N':\n",
        "\t\t\t\tself.agent_dir = 'W'\n",
        "\t\t\telif self.agent_dir == 'W':\n",
        "\t\t\t\tself.agent_dir = 'S'\n",
        "\t\t\telif self.agent_dir == 'S':\n",
        "\t\t\t\tself.agent_dir = 'E'\n",
        "\t\t\telif self.agent_dir == 'E':\n",
        "\t\t\t\tself.agent_dir = 'N'\n",
        "\t\telif action == 'D': # turn left\n",
        "\t\t\tif self.agent_dir == 'N':\n",
        "\t\t\t\tself.agent_dir = 'E'\n",
        "\t\t\telif self.agent_dir == 'W':\n",
        "\t\t\t\tself.agent_dir = 'N'\n",
        "\t\t\telif self.agent_dir == 'S':\n",
        "\t\t\t\tself.agent_dir = 'W'\n",
        "\t\t\telif self.agent_dir == 'E':\n",
        "\t\t\t\tself.agent_dir = 'S'\n",
        "\t\telif action == 'W': # go forward\n",
        "\t\t\t\n",
        "\t\t\t# compute the target position in front of the agent\n",
        "\t\t\t[target_x, target_y] = self.getFacingXY()\n",
        "\t\t\t\n",
        "\t\t\tif self.grid[target_x][target_y] == 0: # if target position is empty, move\n",
        "\t\t\t\tself.agent_x = target_x\n",
        "\t\t\t\tself.agent_y = target_y\n",
        "\t\telif action == 'U': # use / break block\n",
        "\t\t\t# compute the target position in front of the agent\n",
        "\t\t\t[target_x, target_y] = self.getFacingXY()\n",
        "\t\t\t\t\n",
        "\t\t\tif self.grid[target_x][target_y] == 2: # if tree in front\n",
        "\t\t\t\tself.grid[target_x][target_y] = 0 # we clear the tree\n",
        "\t\t\t\tself.inventory['wood'] += 1\n",
        "\t\t\t\tif self.inventory['wood'] <= 2:\n",
        "\t\t\t\t\treward = 10 # learn to chop wood if needed\n",
        "\t\t\telif self.grid[target_x][target_y] == 3: # if rock in front\n",
        "\t\t\t\tself.grid[target_x][target_y] = 0 # we clear the tree\n",
        "\t\t\t\tself.inventory['stone'] += 1\n",
        "\t\t\t\tif self.inventory['stone'] <= 1:\n",
        "\t\t\t\t\treward = 10 # learn to chop stone if needed\n",
        "\t\telif action == 'C': # craft -- need 2 wood and 1 rock\n",
        "\t\t\t[target_x, target_y] = self.getFacingXY()\n",
        "\t\t\tif self.grid[target_x][target_y] == 4: # if craft in front\n",
        "\t\t\t\tif self.inventory['wood'] >= 2 and self.inventory['stone'] >= 1:\n",
        "\t\t\t\t\tself.inventory['pogo'] += 1\n",
        "\t\t\t\t\tself.inventory['wood'] -= 2\n",
        "\t\t\t\t\tself.inventory['stone'] -= 1\n",
        "\t\t\t\t\tdone = True\n",
        "\t\t\t\t\treward = REWARD_DONE\n",
        "\t\treturn [done, reward]\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "def main_dqn():\n",
        "\t# environment\n",
        "\trandom_seed = 10\n",
        "\tenv = microMC(20,10,random_seed)\n",
        "\t\n",
        "\t# sensor\n",
        "\tsensor = LidarSensor(np.pi/8)\n",
        "\t\n",
        "\t# policy network\n",
        "\tactionCnt = 5\n",
        "\tD = 8 * 2 * len(env.object_types) + 2 # how many input neurons\n",
        "\tNUM_HIDDEN = 30\n",
        "\tGAMMA = 0.95\n",
        "\tLEARNING_RATE = 1e-3\n",
        "\tDECAY_RATE = 0.99\n",
        "\tMAX_EPSILON = 0.1\n",
        "\t\n",
        "\tagent = SimplePG(actionCnt,D,NUM_HIDDEN,LEARNING_RATE,GAMMA,DECAY_RATE,MAX_EPSILON,random_seed)\n",
        "\tagent.set_explore_epsilon(MAX_EPSILON)\n",
        " \n",
        "\trewards_history = []\n",
        "\taction_space = ['W','A','D','U','C']\n",
        "\trunning_reward = None\n",
        "  \n",
        "  \n",
        "\tt_step = 0\n",
        "\tepisode = 0\n",
        "\tt_limit = 100\n",
        "\treward_sum = 0\n",
        "  \n",
        "  \n",
        "\n",
        "\twhile True:\n",
        "\t\t#print env.toString()\n",
        "\t\t\n",
        "\t\t# get obseration from sensor\n",
        "\t\tobs = sensor.sense(env)\n",
        "\t\n",
        "\t\t# add inventory observation\n",
        "\t\tobs.append(env.inventory['wood'])\n",
        "\t\tobs.append(env.inventory['stone'])\n",
        "\t\n",
        "\t\t# construct input x \n",
        "\t\tx = np.asarray(obs)\n",
        "\t\t\n",
        "\t\t# act \n",
        "\t\ta = agent.process_step(x,True)\n",
        "\t\t#print(\"Action at t=\"+str(t_step)+\" is \"+action_space[a])\n",
        "\t\t\n",
        "\t\t[done,reward] = env.execute_action(action_space[a])\n",
        "\t\t#print(\"Reward = \"+str(reward))\n",
        "\t\t# give reward\n",
        "\t\tagent.give_reward(reward)\n",
        "\t\treward_sum += reward\n",
        "\t\t\n",
        "\t\tt_step += 1\n",
        "\t\t\n",
        "\t\tif t_step > t_limit:\n",
        "\t\t\t\n",
        "\t\t\t# compute running reward\n",
        "\t\t\trunning_reward = reward_sum if running_reward is None else running_reward * 0.95 + reward_sum * 0.05\n",
        "\t\t\trewards_history.append(running_reward)\n",
        "\t\t\tprint('ep %f: resetting env. episode reward total was %f. running mean: %f' % (episode, reward_sum, running_reward))\n",
        "\t\t\t\n",
        "\t\t\t# finish agent\n",
        "\t\t\t\n",
        "\t\t\t#print(\"\\n\\nfinished episode = \"+str(episode)+\" with \" +str(reward_sum)+\"\\n\")\n",
        "\n",
        "\t\t\t\n",
        "\t\t\tdone = True\n",
        "\t\t\tt_step = 0\n",
        "\t\t\tagent.finish_episode()\n",
        "\t\t\n",
        "\t\t\t# update after every episode\n",
        "\t\t\tagent.update_parameters()\n",
        "\n",
        "\t\t\t# update after every k episoddes\n",
        "\t\t\t#if episode % 5 == 0:\n",
        "\t\t\t#\tagent.update_parameters()\n",
        "\t\t\n",
        "\t\t\t# reset environment\n",
        "\t\t\tepisode += 1\n",
        "\t\t\tenv = microMC(20,10,episode) # this is a bug causing memory leak, ideally env should have a reset function\n",
        "\t\t\treward_sum = 0\n",
        "\t\n",
        "\t\t\t# quit after some number of episodes\n",
        "\t\t\tif episode > 5000:\n",
        "\t\t\t\t break\n",
        "\t\t\t\n",
        "\titerations = range(0, episode, 1)\n",
        "\tplt.plot(iterations, rewards_history)\n",
        "\tplt.ylabel('Average Return'), plt.xlabel('Iterations')\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main_teleop():\n",
        "\t\n",
        "\tenv = microMC(20,10,5)\n",
        "\tprint(env.toString())\n",
        "\t\n",
        "\tsensor = LidarSensor(np.pi/8)\n",
        "\t\n",
        "\t\n",
        "\twhile True:\n",
        "\t\t\n",
        "\t\tobs = sensor.sense(env)\n",
        "\t\t\n",
        "\t\taction = input()\n",
        "\t\t[done,reward] = env.execute_action(action)\n",
        "\t\t\n",
        "\t\tprint(str(done)+\"\\t\"+str(reward))\n",
        "\t\tprint(env.toString())\n",
        "\t\n",
        "\tprint(\"Hello World!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_dqn()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0.000000: resetting env. episode reward total was -90.000000. running mean: -90.000000\n",
            "ep 1.000000: resetting env. episode reward total was -101.000000. running mean: -90.550000\n",
            "ep 2.000000: resetting env. episode reward total was -101.000000. running mean: -91.072500\n",
            "ep 3.000000: resetting env. episode reward total was -79.000000. running mean: -90.468875\n",
            "ep 4.000000: resetting env. episode reward total was -101.000000. running mean: -90.995431\n",
            "ep 5.000000: resetting env. episode reward total was -90.000000. running mean: -90.945660\n",
            "ep 6.000000: resetting env. episode reward total was -101.000000. running mean: -91.448377\n",
            "ep 7.000000: resetting env. episode reward total was -101.000000. running mean: -91.925958\n",
            "ep 8.000000: resetting env. episode reward total was -79.000000. running mean: -91.279660\n",
            "ep 9.000000: resetting env. episode reward total was -101.000000. running mean: -91.765677\n",
            "ep 10.000000: resetting env. episode reward total was -90.000000. running mean: -91.677393\n",
            "ep 11.000000: resetting env. episode reward total was -79.000000. running mean: -91.043523\n",
            "ep 12.000000: resetting env. episode reward total was -90.000000. running mean: -90.991347\n",
            "ep 13.000000: resetting env. episode reward total was -90.000000. running mean: -90.941780\n",
            "ep 14.000000: resetting env. episode reward total was -79.000000. running mean: -90.344691\n",
            "ep 15.000000: resetting env. episode reward total was -90.000000. running mean: -90.327456\n",
            "ep 16.000000: resetting env. episode reward total was -90.000000. running mean: -90.311084\n",
            "ep 17.000000: resetting env. episode reward total was -90.000000. running mean: -90.295529\n",
            "ep 18.000000: resetting env. episode reward total was -79.000000. running mean: -89.730753\n",
            "ep 19.000000: resetting env. episode reward total was -68.000000. running mean: -88.644215\n",
            "ep 20.000000: resetting env. episode reward total was -90.000000. running mean: -88.712005\n",
            "ep 21.000000: resetting env. episode reward total was -90.000000. running mean: -88.776404\n",
            "ep 22.000000: resetting env. episode reward total was -79.000000. running mean: -88.287584\n",
            "ep 23.000000: resetting env. episode reward total was -79.000000. running mean: -87.823205\n",
            "ep 24.000000: resetting env. episode reward total was -79.000000. running mean: -87.382045\n",
            "ep 25.000000: resetting env. episode reward total was -90.000000. running mean: -87.512942\n",
            "ep 26.000000: resetting env. episode reward total was -79.000000. running mean: -87.087295\n",
            "ep 27.000000: resetting env. episode reward total was -79.000000. running mean: -86.682931\n",
            "ep 28.000000: resetting env. episode reward total was -101.000000. running mean: -87.398784\n",
            "ep 29.000000: resetting env. episode reward total was -90.000000. running mean: -87.528845\n",
            "ep 30.000000: resetting env. episode reward total was -90.000000. running mean: -87.652403\n",
            "ep 31.000000: resetting env. episode reward total was -79.000000. running mean: -87.219782\n",
            "ep 32.000000: resetting env. episode reward total was -68.000000. running mean: -86.258793\n",
            "ep 33.000000: resetting env. episode reward total was -90.000000. running mean: -86.445854\n",
            "ep 34.000000: resetting env. episode reward total was -101.000000. running mean: -87.173561\n",
            "ep 35.000000: resetting env. episode reward total was -101.000000. running mean: -87.864883\n",
            "ep 36.000000: resetting env. episode reward total was -79.000000. running mean: -87.421639\n",
            "ep 37.000000: resetting env. episode reward total was -101.000000. running mean: -88.100557\n",
            "ep 38.000000: resetting env. episode reward total was -101.000000. running mean: -88.745529\n",
            "ep 39.000000: resetting env. episode reward total was -101.000000. running mean: -89.358253\n",
            "ep 40.000000: resetting env. episode reward total was -90.000000. running mean: -89.390340\n",
            "ep 41.000000: resetting env. episode reward total was -79.000000. running mean: -88.870823\n",
            "ep 42.000000: resetting env. episode reward total was -90.000000. running mean: -88.927282\n",
            "ep 43.000000: resetting env. episode reward total was -79.000000. running mean: -88.430918\n",
            "ep 44.000000: resetting env. episode reward total was -68.000000. running mean: -87.409372\n",
            "ep 45.000000: resetting env. episode reward total was -90.000000. running mean: -87.538903\n",
            "ep 46.000000: resetting env. episode reward total was -101.000000. running mean: -88.211958\n",
            "ep 47.000000: resetting env. episode reward total was -79.000000. running mean: -87.751360\n",
            "ep 48.000000: resetting env. episode reward total was -101.000000. running mean: -88.413792\n",
            "ep 49.000000: resetting env. episode reward total was -90.000000. running mean: -88.493103\n",
            "ep 50.000000: resetting env. episode reward total was -79.000000. running mean: -88.018447\n",
            "ep 51.000000: resetting env. episode reward total was -79.000000. running mean: -87.567525\n",
            "ep 52.000000: resetting env. episode reward total was -68.000000. running mean: -86.589149\n",
            "ep 53.000000: resetting env. episode reward total was -79.000000. running mean: -86.209691\n",
            "ep 54.000000: resetting env. episode reward total was -79.000000. running mean: -85.849207\n",
            "ep 55.000000: resetting env. episode reward total was -101.000000. running mean: -86.606746\n",
            "ep 56.000000: resetting env. episode reward total was -90.000000. running mean: -86.776409\n",
            "ep 57.000000: resetting env. episode reward total was -90.000000. running mean: -86.937589\n",
            "ep 58.000000: resetting env. episode reward total was -101.000000. running mean: -87.640709\n",
            "ep 59.000000: resetting env. episode reward total was -68.000000. running mean: -86.658674\n",
            "ep 60.000000: resetting env. episode reward total was -90.000000. running mean: -86.825740\n",
            "ep 61.000000: resetting env. episode reward total was -90.000000. running mean: -86.984453\n",
            "ep 62.000000: resetting env. episode reward total was -79.000000. running mean: -86.585230\n",
            "ep 63.000000: resetting env. episode reward total was -90.000000. running mean: -86.755969\n",
            "ep 64.000000: resetting env. episode reward total was -79.000000. running mean: -86.368170\n",
            "ep 65.000000: resetting env. episode reward total was -68.000000. running mean: -85.449762\n",
            "ep 66.000000: resetting env. episode reward total was -90.000000. running mean: -85.677274\n",
            "ep 67.000000: resetting env. episode reward total was -90.000000. running mean: -85.893410\n",
            "ep 68.000000: resetting env. episode reward total was -68.000000. running mean: -84.998740\n",
            "ep 69.000000: resetting env. episode reward total was -79.000000. running mean: -84.698803\n",
            "ep 70.000000: resetting env. episode reward total was -79.000000. running mean: -84.413863\n",
            "ep 71.000000: resetting env. episode reward total was -90.000000. running mean: -84.693169\n",
            "ep 72.000000: resetting env. episode reward total was -90.000000. running mean: -84.958511\n",
            "ep 73.000000: resetting env. episode reward total was -68.000000. running mean: -84.110585\n",
            "ep 74.000000: resetting env. episode reward total was -101.000000. running mean: -84.955056\n",
            "ep 75.000000: resetting env. episode reward total was -90.000000. running mean: -85.207303\n",
            "ep 76.000000: resetting env. episode reward total was -90.000000. running mean: -85.446938\n",
            "ep 77.000000: resetting env. episode reward total was -79.000000. running mean: -85.124591\n",
            "ep 78.000000: resetting env. episode reward total was -101.000000. running mean: -85.918362\n",
            "ep 79.000000: resetting env. episode reward total was -90.000000. running mean: -86.122444\n",
            "ep 80.000000: resetting env. episode reward total was -79.000000. running mean: -85.766321\n",
            "ep 81.000000: resetting env. episode reward total was -79.000000. running mean: -85.428005\n",
            "ep 82.000000: resetting env. episode reward total was -101.000000. running mean: -86.206605\n",
            "ep 83.000000: resetting env. episode reward total was -90.000000. running mean: -86.396275\n",
            "ep 84.000000: resetting env. episode reward total was -90.000000. running mean: -86.576461\n",
            "ep 85.000000: resetting env. episode reward total was -90.000000. running mean: -86.747638\n",
            "ep 86.000000: resetting env. episode reward total was -90.000000. running mean: -86.910256\n",
            "ep 87.000000: resetting env. episode reward total was -79.000000. running mean: -86.514743\n",
            "ep 88.000000: resetting env. episode reward total was -68.000000. running mean: -85.589006\n",
            "ep 89.000000: resetting env. episode reward total was -101.000000. running mean: -86.359556\n",
            "ep 90.000000: resetting env. episode reward total was -90.000000. running mean: -86.541578\n",
            "ep 91.000000: resetting env. episode reward total was -90.000000. running mean: -86.714499\n",
            "ep 92.000000: resetting env. episode reward total was -79.000000. running mean: -86.328774\n",
            "ep 93.000000: resetting env. episode reward total was -79.000000. running mean: -85.962335\n",
            "ep 94.000000: resetting env. episode reward total was -101.000000. running mean: -86.714219\n",
            "ep 95.000000: resetting env. episode reward total was -79.000000. running mean: -86.328508\n",
            "ep 96.000000: resetting env. episode reward total was -90.000000. running mean: -86.512082\n",
            "ep 97.000000: resetting env. episode reward total was -90.000000. running mean: -86.686478\n",
            "ep 98.000000: resetting env. episode reward total was -101.000000. running mean: -87.402154\n",
            "ep 99.000000: resetting env. episode reward total was -90.000000. running mean: -87.532047\n",
            "ep 100.000000: resetting env. episode reward total was -79.000000. running mean: -87.105444\n",
            "ep 101.000000: resetting env. episode reward total was -79.000000. running mean: -86.700172\n",
            "ep 102.000000: resetting env. episode reward total was -101.000000. running mean: -87.415163\n",
            "ep 103.000000: resetting env. episode reward total was -90.000000. running mean: -87.544405\n",
            "ep 104.000000: resetting env. episode reward total was -90.000000. running mean: -87.667185\n",
            "ep 105.000000: resetting env. episode reward total was -79.000000. running mean: -87.233826\n",
            "ep 106.000000: resetting env. episode reward total was -68.000000. running mean: -86.272135\n",
            "ep 107.000000: resetting env. episode reward total was -90.000000. running mean: -86.458528\n",
            "ep 108.000000: resetting env. episode reward total was -79.000000. running mean: -86.085601\n",
            "ep 109.000000: resetting env. episode reward total was -101.000000. running mean: -86.831321\n",
            "ep 110.000000: resetting env. episode reward total was -90.000000. running mean: -86.989755\n",
            "ep 111.000000: resetting env. episode reward total was -90.000000. running mean: -87.140267\n",
            "ep 112.000000: resetting env. episode reward total was -79.000000. running mean: -86.733254\n",
            "ep 113.000000: resetting env. episode reward total was -90.000000. running mean: -86.896591\n",
            "ep 114.000000: resetting env. episode reward total was -90.000000. running mean: -87.051762\n",
            "ep 115.000000: resetting env. episode reward total was -90.000000. running mean: -87.199174\n",
            "ep 116.000000: resetting env. episode reward total was -68.000000. running mean: -86.239215\n",
            "ep 117.000000: resetting env. episode reward total was -79.000000. running mean: -85.877254\n",
            "ep 118.000000: resetting env. episode reward total was -90.000000. running mean: -86.083392\n",
            "ep 119.000000: resetting env. episode reward total was -90.000000. running mean: -86.279222\n",
            "ep 120.000000: resetting env. episode reward total was -90.000000. running mean: -86.465261\n",
            "ep 121.000000: resetting env. episode reward total was -79.000000. running mean: -86.091998\n",
            "ep 122.000000: resetting env. episode reward total was -90.000000. running mean: -86.287398\n",
            "ep 123.000000: resetting env. episode reward total was -68.000000. running mean: -85.373028\n",
            "ep 124.000000: resetting env. episode reward total was -90.000000. running mean: -85.604377\n",
            "ep 125.000000: resetting env. episode reward total was -90.000000. running mean: -85.824158\n",
            "ep 126.000000: resetting env. episode reward total was -79.000000. running mean: -85.482950\n",
            "ep 127.000000: resetting env. episode reward total was -101.000000. running mean: -86.258802\n",
            "ep 128.000000: resetting env. episode reward total was -79.000000. running mean: -85.895862\n",
            "ep 129.000000: resetting env. episode reward total was -79.000000. running mean: -85.551069\n",
            "ep 130.000000: resetting env. episode reward total was -101.000000. running mean: -86.323516\n",
            "ep 131.000000: resetting env. episode reward total was -101.000000. running mean: -87.057340\n",
            "ep 132.000000: resetting env. episode reward total was -68.000000. running mean: -86.104473\n",
            "ep 133.000000: resetting env. episode reward total was -101.000000. running mean: -86.849249\n",
            "ep 134.000000: resetting env. episode reward total was -68.000000. running mean: -85.906787\n",
            "ep 135.000000: resetting env. episode reward total was -101.000000. running mean: -86.661448\n",
            "ep 136.000000: resetting env. episode reward total was -90.000000. running mean: -86.828375\n",
            "ep 137.000000: resetting env. episode reward total was -79.000000. running mean: -86.436956\n",
            "ep 138.000000: resetting env. episode reward total was -90.000000. running mean: -86.615109\n",
            "ep 139.000000: resetting env. episode reward total was -90.000000. running mean: -86.784353\n",
            "ep 140.000000: resetting env. episode reward total was -68.000000. running mean: -85.845135\n",
            "ep 141.000000: resetting env. episode reward total was -79.000000. running mean: -85.502879\n",
            "ep 142.000000: resetting env. episode reward total was -79.000000. running mean: -85.177735\n",
            "ep 143.000000: resetting env. episode reward total was -79.000000. running mean: -84.868848\n",
            "ep 144.000000: resetting env. episode reward total was -90.000000. running mean: -85.125406\n",
            "ep 145.000000: resetting env. episode reward total was -90.000000. running mean: -85.369135\n",
            "ep 146.000000: resetting env. episode reward total was -68.000000. running mean: -84.500679\n",
            "ep 147.000000: resetting env. episode reward total was -90.000000. running mean: -84.775645\n",
            "ep 148.000000: resetting env. episode reward total was -90.000000. running mean: -85.036862\n",
            "ep 149.000000: resetting env. episode reward total was -79.000000. running mean: -84.735019\n",
            "ep 150.000000: resetting env. episode reward total was -90.000000. running mean: -84.998268\n",
            "ep 151.000000: resetting env. episode reward total was -90.000000. running mean: -85.248355\n",
            "ep 152.000000: resetting env. episode reward total was -79.000000. running mean: -84.935937\n",
            "ep 153.000000: resetting env. episode reward total was -68.000000. running mean: -84.089140\n",
            "ep 154.000000: resetting env. episode reward total was -79.000000. running mean: -83.834683\n",
            "ep 155.000000: resetting env. episode reward total was -79.000000. running mean: -83.592949\n",
            "ep 156.000000: resetting env. episode reward total was -79.000000. running mean: -83.363302\n",
            "ep 157.000000: resetting env. episode reward total was -101.000000. running mean: -84.245137\n",
            "ep 158.000000: resetting env. episode reward total was -79.000000. running mean: -83.982880\n",
            "ep 159.000000: resetting env. episode reward total was -90.000000. running mean: -84.283736\n",
            "ep 160.000000: resetting env. episode reward total was -90.000000. running mean: -84.569549\n",
            "ep 161.000000: resetting env. episode reward total was -90.000000. running mean: -84.841072\n",
            "ep 162.000000: resetting env. episode reward total was -90.000000. running mean: -85.099018\n",
            "ep 163.000000: resetting env. episode reward total was -90.000000. running mean: -85.344067\n",
            "ep 164.000000: resetting env. episode reward total was -68.000000. running mean: -84.476864\n",
            "ep 165.000000: resetting env. episode reward total was -90.000000. running mean: -84.753021\n",
            "ep 166.000000: resetting env. episode reward total was -68.000000. running mean: -83.915369\n",
            "ep 167.000000: resetting env. episode reward total was -101.000000. running mean: -84.769601\n",
            "ep 168.000000: resetting env. episode reward total was -90.000000. running mean: -85.031121\n",
            "ep 169.000000: resetting env. episode reward total was -101.000000. running mean: -85.829565\n",
            "ep 170.000000: resetting env. episode reward total was -101.000000. running mean: -86.588087\n",
            "ep 171.000000: resetting env. episode reward total was -79.000000. running mean: -86.208682\n",
            "ep 172.000000: resetting env. episode reward total was -90.000000. running mean: -86.398248\n",
            "ep 173.000000: resetting env. episode reward total was -90.000000. running mean: -86.578336\n",
            "ep 174.000000: resetting env. episode reward total was -79.000000. running mean: -86.199419\n",
            "ep 175.000000: resetting env. episode reward total was -79.000000. running mean: -85.839448\n",
            "ep 176.000000: resetting env. episode reward total was -68.000000. running mean: -84.947476\n",
            "ep 177.000000: resetting env. episode reward total was -101.000000. running mean: -85.750102\n",
            "ep 178.000000: resetting env. episode reward total was -90.000000. running mean: -85.962597\n",
            "ep 179.000000: resetting env. episode reward total was -68.000000. running mean: -85.064467\n",
            "ep 180.000000: resetting env. episode reward total was -79.000000. running mean: -84.761244\n",
            "ep 181.000000: resetting env. episode reward total was -79.000000. running mean: -84.473181\n",
            "ep 182.000000: resetting env. episode reward total was -90.000000. running mean: -84.749522\n",
            "ep 183.000000: resetting env. episode reward total was -101.000000. running mean: -85.562046\n",
            "ep 184.000000: resetting env. episode reward total was -90.000000. running mean: -85.783944\n",
            "ep 185.000000: resetting env. episode reward total was -79.000000. running mean: -85.444747\n",
            "ep 186.000000: resetting env. episode reward total was -68.000000. running mean: -84.572509\n",
            "ep 187.000000: resetting env. episode reward total was -90.000000. running mean: -84.843884\n",
            "ep 188.000000: resetting env. episode reward total was -68.000000. running mean: -84.001690\n",
            "ep 189.000000: resetting env. episode reward total was -68.000000. running mean: -83.201605\n",
            "ep 190.000000: resetting env. episode reward total was -90.000000. running mean: -83.541525\n",
            "ep 191.000000: resetting env. episode reward total was -90.000000. running mean: -83.864449\n",
            "ep 192.000000: resetting env. episode reward total was -79.000000. running mean: -83.621226\n",
            "ep 193.000000: resetting env. episode reward total was -79.000000. running mean: -83.390165\n",
            "ep 194.000000: resetting env. episode reward total was -68.000000. running mean: -82.620657\n",
            "ep 195.000000: resetting env. episode reward total was -68.000000. running mean: -81.889624\n",
            "ep 196.000000: resetting env. episode reward total was -79.000000. running mean: -81.745143\n",
            "ep 197.000000: resetting env. episode reward total was -90.000000. running mean: -82.157886\n",
            "ep 198.000000: resetting env. episode reward total was -68.000000. running mean: -81.449991\n",
            "ep 199.000000: resetting env. episode reward total was -90.000000. running mean: -81.877492\n",
            "ep 200.000000: resetting env. episode reward total was -79.000000. running mean: -81.733617\n",
            "ep 201.000000: resetting env. episode reward total was -90.000000. running mean: -82.146936\n",
            "ep 202.000000: resetting env. episode reward total was -90.000000. running mean: -82.539589\n",
            "ep 203.000000: resetting env. episode reward total was -79.000000. running mean: -82.362610\n",
            "ep 204.000000: resetting env. episode reward total was -68.000000. running mean: -81.644479\n",
            "ep 205.000000: resetting env. episode reward total was -79.000000. running mean: -81.512256\n",
            "ep 206.000000: resetting env. episode reward total was -79.000000. running mean: -81.386643\n",
            "ep 207.000000: resetting env. episode reward total was -90.000000. running mean: -81.817311\n",
            "ep 208.000000: resetting env. episode reward total was -68.000000. running mean: -81.126445\n",
            "ep 209.000000: resetting env. episode reward total was -79.000000. running mean: -81.020123\n",
            "ep 210.000000: resetting env. episode reward total was -90.000000. running mean: -81.469117\n",
            "ep 211.000000: resetting env. episode reward total was -101.000000. running mean: -82.445661\n",
            "ep 212.000000: resetting env. episode reward total was -79.000000. running mean: -82.273378\n",
            "ep 213.000000: resetting env. episode reward total was -68.000000. running mean: -81.559709\n",
            "ep 214.000000: resetting env. episode reward total was -79.000000. running mean: -81.431723\n",
            "ep 215.000000: resetting env. episode reward total was -79.000000. running mean: -81.310137\n",
            "ep 216.000000: resetting env. episode reward total was -68.000000. running mean: -80.644630\n",
            "ep 217.000000: resetting env. episode reward total was -90.000000. running mean: -81.112399\n",
            "ep 218.000000: resetting env. episode reward total was -79.000000. running mean: -81.006779\n",
            "ep 219.000000: resetting env. episode reward total was -90.000000. running mean: -81.456440\n",
            "ep 220.000000: resetting env. episode reward total was -79.000000. running mean: -81.333618\n",
            "ep 221.000000: resetting env. episode reward total was -79.000000. running mean: -81.216937\n",
            "ep 222.000000: resetting env. episode reward total was -79.000000. running mean: -81.106090\n",
            "ep 223.000000: resetting env. episode reward total was -90.000000. running mean: -81.550786\n",
            "ep 224.000000: resetting env. episode reward total was -90.000000. running mean: -81.973246\n",
            "ep 225.000000: resetting env. episode reward total was -68.000000. running mean: -81.274584\n",
            "ep 226.000000: resetting env. episode reward total was -79.000000. running mean: -81.160855\n",
            "ep 227.000000: resetting env. episode reward total was -68.000000. running mean: -80.502812\n",
            "ep 228.000000: resetting env. episode reward total was -79.000000. running mean: -80.427672\n",
            "ep 229.000000: resetting env. episode reward total was -79.000000. running mean: -80.356288\n",
            "ep 230.000000: resetting env. episode reward total was -90.000000. running mean: -80.838474\n",
            "ep 231.000000: resetting env. episode reward total was -101.000000. running mean: -81.846550\n",
            "ep 232.000000: resetting env. episode reward total was -79.000000. running mean: -81.704222\n",
            "ep 233.000000: resetting env. episode reward total was -68.000000. running mean: -81.019011\n",
            "ep 234.000000: resetting env. episode reward total was -101.000000. running mean: -82.018061\n",
            "ep 235.000000: resetting env. episode reward total was -68.000000. running mean: -81.317158\n",
            "ep 236.000000: resetting env. episode reward total was -90.000000. running mean: -81.751300\n",
            "ep 237.000000: resetting env. episode reward total was -90.000000. running mean: -82.163735\n",
            "ep 238.000000: resetting env. episode reward total was -79.000000. running mean: -82.005548\n",
            "ep 239.000000: resetting env. episode reward total was -79.000000. running mean: -81.855271\n",
            "ep 240.000000: resetting env. episode reward total was -79.000000. running mean: -81.712507\n",
            "ep 241.000000: resetting env. episode reward total was -79.000000. running mean: -81.576882\n",
            "ep 242.000000: resetting env. episode reward total was -90.000000. running mean: -81.998038\n",
            "ep 243.000000: resetting env. episode reward total was -90.000000. running mean: -82.398136\n",
            "ep 244.000000: resetting env. episode reward total was -79.000000. running mean: -82.228229\n",
            "ep 245.000000: resetting env. episode reward total was -90.000000. running mean: -82.616818\n",
            "ep 246.000000: resetting env. episode reward total was -90.000000. running mean: -82.985977\n",
            "ep 247.000000: resetting env. episode reward total was -79.000000. running mean: -82.786678\n",
            "ep 248.000000: resetting env. episode reward total was -68.000000. running mean: -82.047344\n",
            "ep 249.000000: resetting env. episode reward total was -68.000000. running mean: -81.344977\n",
            "ep 250.000000: resetting env. episode reward total was -68.000000. running mean: -80.677728\n",
            "ep 251.000000: resetting env. episode reward total was -79.000000. running mean: -80.593842\n",
            "ep 252.000000: resetting env. episode reward total was -79.000000. running mean: -80.514149\n",
            "ep 253.000000: resetting env. episode reward total was -68.000000. running mean: -79.888442\n",
            "ep 254.000000: resetting env. episode reward total was -79.000000. running mean: -79.844020\n",
            "ep 255.000000: resetting env. episode reward total was -90.000000. running mean: -80.351819\n",
            "ep 256.000000: resetting env. episode reward total was -79.000000. running mean: -80.284228\n",
            "ep 257.000000: resetting env. episode reward total was -79.000000. running mean: -80.220017\n",
            "ep 258.000000: resetting env. episode reward total was -79.000000. running mean: -80.159016\n",
            "ep 259.000000: resetting env. episode reward total was -90.000000. running mean: -80.651065\n",
            "ep 260.000000: resetting env. episode reward total was -79.000000. running mean: -80.568512\n",
            "ep 261.000000: resetting env. episode reward total was -68.000000. running mean: -79.940086\n",
            "ep 262.000000: resetting env. episode reward total was -68.000000. running mean: -79.343082\n",
            "ep 263.000000: resetting env. episode reward total was -68.000000. running mean: -78.775928\n",
            "ep 264.000000: resetting env. episode reward total was -90.000000. running mean: -79.337131\n",
            "ep 265.000000: resetting env. episode reward total was -79.000000. running mean: -79.320275\n",
            "ep 266.000000: resetting env. episode reward total was -90.000000. running mean: -79.854261\n",
            "ep 267.000000: resetting env. episode reward total was -79.000000. running mean: -79.811548\n",
            "ep 268.000000: resetting env. episode reward total was -79.000000. running mean: -79.770971\n",
            "ep 269.000000: resetting env. episode reward total was -68.000000. running mean: -79.182422\n",
            "ep 270.000000: resetting env. episode reward total was -90.000000. running mean: -79.723301\n",
            "ep 271.000000: resetting env. episode reward total was -79.000000. running mean: -79.687136\n",
            "ep 272.000000: resetting env. episode reward total was -101.000000. running mean: -80.752779\n",
            "ep 273.000000: resetting env. episode reward total was -90.000000. running mean: -81.215140\n",
            "ep 274.000000: resetting env. episode reward total was -79.000000. running mean: -81.104383\n",
            "ep 275.000000: resetting env. episode reward total was -79.000000. running mean: -80.999164\n",
            "ep 276.000000: resetting env. episode reward total was -68.000000. running mean: -80.349206\n",
            "ep 277.000000: resetting env. episode reward total was -68.000000. running mean: -79.731745\n",
            "ep 278.000000: resetting env. episode reward total was -79.000000. running mean: -79.695158\n",
            "ep 279.000000: resetting env. episode reward total was -79.000000. running mean: -79.660400\n",
            "ep 280.000000: resetting env. episode reward total was -79.000000. running mean: -79.627380\n",
            "ep 281.000000: resetting env. episode reward total was -68.000000. running mean: -79.046011\n",
            "ep 282.000000: resetting env. episode reward total was -90.000000. running mean: -79.593711\n",
            "ep 283.000000: resetting env. episode reward total was -90.000000. running mean: -80.114025\n",
            "ep 284.000000: resetting env. episode reward total was -68.000000. running mean: -79.508324\n",
            "ep 285.000000: resetting env. episode reward total was -90.000000. running mean: -80.032908\n",
            "ep 286.000000: resetting env. episode reward total was -79.000000. running mean: -79.981262\n",
            "ep 287.000000: resetting env. episode reward total was -90.000000. running mean: -80.482199\n",
            "ep 288.000000: resetting env. episode reward total was -79.000000. running mean: -80.408089\n",
            "ep 289.000000: resetting env. episode reward total was -79.000000. running mean: -80.337685\n",
            "ep 290.000000: resetting env. episode reward total was -79.000000. running mean: -80.270801\n",
            "ep 291.000000: resetting env. episode reward total was -68.000000. running mean: -79.657261\n",
            "ep 292.000000: resetting env. episode reward total was -79.000000. running mean: -79.624398\n",
            "ep 293.000000: resetting env. episode reward total was -79.000000. running mean: -79.593178\n",
            "ep 294.000000: resetting env. episode reward total was 955.000000. running mean: -27.863519\n",
            "ep 295.000000: resetting env. episode reward total was -68.000000. running mean: -29.870343\n",
            "ep 296.000000: resetting env. episode reward total was -68.000000. running mean: -31.776826\n",
            "ep 297.000000: resetting env. episode reward total was -90.000000. running mean: -34.687984\n",
            "ep 298.000000: resetting env. episode reward total was -79.000000. running mean: -36.903585\n",
            "ep 299.000000: resetting env. episode reward total was -79.000000. running mean: -39.008406\n",
            "ep 300.000000: resetting env. episode reward total was -101.000000. running mean: -42.107986\n",
            "ep 301.000000: resetting env. episode reward total was -90.000000. running mean: -44.502586\n",
            "ep 302.000000: resetting env. episode reward total was -68.000000. running mean: -45.677457\n",
            "ep 303.000000: resetting env. episode reward total was -68.000000. running mean: -46.793584\n",
            "ep 304.000000: resetting env. episode reward total was -68.000000. running mean: -47.853905\n",
            "ep 305.000000: resetting env. episode reward total was -79.000000. running mean: -49.411210\n",
            "ep 306.000000: resetting env. episode reward total was -79.000000. running mean: -50.890649\n",
            "ep 307.000000: resetting env. episode reward total was -68.000000. running mean: -51.746117\n",
            "ep 308.000000: resetting env. episode reward total was -90.000000. running mean: -53.658811\n",
            "ep 309.000000: resetting env. episode reward total was -68.000000. running mean: -54.375870\n",
            "ep 310.000000: resetting env. episode reward total was -68.000000. running mean: -55.057077\n",
            "ep 311.000000: resetting env. episode reward total was -79.000000. running mean: -56.254223\n",
            "ep 312.000000: resetting env. episode reward total was -90.000000. running mean: -57.941512\n",
            "ep 313.000000: resetting env. episode reward total was -79.000000. running mean: -58.994436\n",
            "ep 314.000000: resetting env. episode reward total was -68.000000. running mean: -59.444714\n",
            "ep 315.000000: resetting env. episode reward total was -68.000000. running mean: -59.872479\n",
            "ep 316.000000: resetting env. episode reward total was -79.000000. running mean: -60.828855\n",
            "ep 317.000000: resetting env. episode reward total was -68.000000. running mean: -61.187412\n",
            "ep 318.000000: resetting env. episode reward total was -79.000000. running mean: -62.078041\n",
            "ep 319.000000: resetting env. episode reward total was -68.000000. running mean: -62.374139\n",
            "ep 320.000000: resetting env. episode reward total was -79.000000. running mean: -63.205432\n",
            "ep 321.000000: resetting env. episode reward total was -90.000000. running mean: -64.545161\n",
            "ep 322.000000: resetting env. episode reward total was -79.000000. running mean: -65.267903\n",
            "ep 323.000000: resetting env. episode reward total was -68.000000. running mean: -65.404508\n",
            "ep 324.000000: resetting env. episode reward total was -79.000000. running mean: -66.084282\n",
            "ep 325.000000: resetting env. episode reward total was -68.000000. running mean: -66.180068\n",
            "ep 326.000000: resetting env. episode reward total was -79.000000. running mean: -66.821065\n",
            "ep 327.000000: resetting env. episode reward total was -68.000000. running mean: -66.880011\n",
            "ep 328.000000: resetting env. episode reward total was -90.000000. running mean: -68.036011\n",
            "ep 329.000000: resetting env. episode reward total was -68.000000. running mean: -68.034210\n",
            "ep 330.000000: resetting env. episode reward total was -79.000000. running mean: -68.582500\n",
            "ep 331.000000: resetting env. episode reward total was -90.000000. running mean: -69.653375\n",
            "ep 332.000000: resetting env. episode reward total was -68.000000. running mean: -69.570706\n",
            "ep 333.000000: resetting env. episode reward total was -90.000000. running mean: -70.592171\n",
            "ep 334.000000: resetting env. episode reward total was -68.000000. running mean: -70.462562\n",
            "ep 335.000000: resetting env. episode reward total was -79.000000. running mean: -70.889434\n",
            "ep 336.000000: resetting env. episode reward total was -68.000000. running mean: -70.744962\n",
            "ep 337.000000: resetting env. episode reward total was -79.000000. running mean: -71.157714\n",
            "ep 338.000000: resetting env. episode reward total was -79.000000. running mean: -71.549829\n",
            "ep 339.000000: resetting env. episode reward total was -79.000000. running mean: -71.922337\n",
            "ep 340.000000: resetting env. episode reward total was -79.000000. running mean: -72.276220\n",
            "ep 341.000000: resetting env. episode reward total was -79.000000. running mean: -72.612409\n",
            "ep 342.000000: resetting env. episode reward total was -68.000000. running mean: -72.381789\n",
            "ep 343.000000: resetting env. episode reward total was -79.000000. running mean: -72.712699\n",
            "ep 344.000000: resetting env. episode reward total was -79.000000. running mean: -73.027064\n",
            "ep 345.000000: resetting env. episode reward total was -79.000000. running mean: -73.325711\n",
            "ep 346.000000: resetting env. episode reward total was -90.000000. running mean: -74.159426\n",
            "ep 347.000000: resetting env. episode reward total was -90.000000. running mean: -74.951454\n",
            "ep 348.000000: resetting env. episode reward total was -79.000000. running mean: -75.153882\n",
            "ep 349.000000: resetting env. episode reward total was -68.000000. running mean: -74.796188\n",
            "ep 350.000000: resetting env. episode reward total was -101.000000. running mean: -76.106378\n",
            "ep 351.000000: resetting env. episode reward total was -79.000000. running mean: -76.251059\n",
            "ep 352.000000: resetting env. episode reward total was -68.000000. running mean: -75.838506\n",
            "ep 353.000000: resetting env. episode reward total was -90.000000. running mean: -76.546581\n",
            "ep 354.000000: resetting env. episode reward total was -90.000000. running mean: -77.219252\n",
            "ep 355.000000: resetting env. episode reward total was -68.000000. running mean: -76.758289\n",
            "ep 356.000000: resetting env. episode reward total was -90.000000. running mean: -77.420375\n",
            "ep 357.000000: resetting env. episode reward total was -79.000000. running mean: -77.499356\n",
            "ep 358.000000: resetting env. episode reward total was -68.000000. running mean: -77.024388\n",
            "ep 359.000000: resetting env. episode reward total was -68.000000. running mean: -76.573169\n",
            "ep 360.000000: resetting env. episode reward total was -79.000000. running mean: -76.694510\n",
            "ep 361.000000: resetting env. episode reward total was -68.000000. running mean: -76.259785\n",
            "ep 362.000000: resetting env. episode reward total was -68.000000. running mean: -75.846796\n",
            "ep 363.000000: resetting env. episode reward total was -90.000000. running mean: -76.554456\n",
            "ep 364.000000: resetting env. episode reward total was -68.000000. running mean: -76.126733\n",
            "ep 365.000000: resetting env. episode reward total was -68.000000. running mean: -75.720396\n",
            "ep 366.000000: resetting env. episode reward total was -68.000000. running mean: -75.334377\n",
            "ep 367.000000: resetting env. episode reward total was -79.000000. running mean: -75.517658\n",
            "ep 368.000000: resetting env. episode reward total was -101.000000. running mean: -76.791775\n",
            "ep 369.000000: resetting env. episode reward total was -68.000000. running mean: -76.352186\n",
            "ep 370.000000: resetting env. episode reward total was -68.000000. running mean: -75.934577\n",
            "ep 371.000000: resetting env. episode reward total was -68.000000. running mean: -75.537848\n",
            "ep 372.000000: resetting env. episode reward total was -79.000000. running mean: -75.710956\n",
            "ep 373.000000: resetting env. episode reward total was -79.000000. running mean: -75.875408\n",
            "ep 374.000000: resetting env. episode reward total was -68.000000. running mean: -75.481637\n",
            "ep 375.000000: resetting env. episode reward total was -79.000000. running mean: -75.657556\n",
            "ep 376.000000: resetting env. episode reward total was -68.000000. running mean: -75.274678\n",
            "ep 377.000000: resetting env. episode reward total was -68.000000. running mean: -74.910944\n",
            "ep 378.000000: resetting env. episode reward total was -79.000000. running mean: -75.115397\n",
            "ep 379.000000: resetting env. episode reward total was -90.000000. running mean: -75.859627\n",
            "ep 380.000000: resetting env. episode reward total was -79.000000. running mean: -76.016646\n",
            "ep 381.000000: resetting env. episode reward total was -90.000000. running mean: -76.715813\n",
            "ep 382.000000: resetting env. episode reward total was -68.000000. running mean: -76.280023\n",
            "ep 383.000000: resetting env. episode reward total was -79.000000. running mean: -76.416021\n",
            "ep 384.000000: resetting env. episode reward total was -79.000000. running mean: -76.545220\n",
            "ep 385.000000: resetting env. episode reward total was -68.000000. running mean: -76.117959\n",
            "ep 386.000000: resetting env. episode reward total was -68.000000. running mean: -75.712061\n",
            "ep 387.000000: resetting env. episode reward total was -68.000000. running mean: -75.326458\n",
            "ep 388.000000: resetting env. episode reward total was -68.000000. running mean: -74.960135\n",
            "ep 389.000000: resetting env. episode reward total was -79.000000. running mean: -75.162129\n",
            "ep 390.000000: resetting env. episode reward total was -79.000000. running mean: -75.354022\n",
            "ep 391.000000: resetting env. episode reward total was -79.000000. running mean: -75.536321\n",
            "ep 392.000000: resetting env. episode reward total was -90.000000. running mean: -76.259505\n",
            "ep 393.000000: resetting env. episode reward total was -68.000000. running mean: -75.846530\n",
            "ep 394.000000: resetting env. episode reward total was -79.000000. running mean: -76.004203\n",
            "ep 395.000000: resetting env. episode reward total was 933.000000. running mean: -25.553993\n",
            "ep 396.000000: resetting env. episode reward total was -68.000000. running mean: -27.676293\n",
            "ep 397.000000: resetting env. episode reward total was -79.000000. running mean: -30.242479\n",
            "ep 398.000000: resetting env. episode reward total was -68.000000. running mean: -32.130355\n",
            "ep 399.000000: resetting env. episode reward total was -68.000000. running mean: -33.923837\n",
            "ep 400.000000: resetting env. episode reward total was -68.000000. running mean: -35.627645\n",
            "ep 401.000000: resetting env. episode reward total was -68.000000. running mean: -37.246263\n",
            "ep 402.000000: resetting env. episode reward total was -68.000000. running mean: -38.783950\n",
            "ep 403.000000: resetting env. episode reward total was -79.000000. running mean: -40.794752\n",
            "ep 404.000000: resetting env. episode reward total was -68.000000. running mean: -42.155015\n",
            "ep 405.000000: resetting env. episode reward total was -68.000000. running mean: -43.447264\n",
            "ep 406.000000: resetting env. episode reward total was -68.000000. running mean: -44.674901\n",
            "ep 407.000000: resetting env. episode reward total was -68.000000. running mean: -45.841156\n",
            "ep 408.000000: resetting env. episode reward total was -68.000000. running mean: -46.949098\n",
            "ep 409.000000: resetting env. episode reward total was -68.000000. running mean: -48.001643\n",
            "ep 410.000000: resetting env. episode reward total was -79.000000. running mean: -49.551561\n",
            "ep 411.000000: resetting env. episode reward total was -68.000000. running mean: -50.473983\n",
            "ep 412.000000: resetting env. episode reward total was -68.000000. running mean: -51.350284\n",
            "ep 413.000000: resetting env. episode reward total was -68.000000. running mean: -52.182770\n",
            "ep 414.000000: resetting env. episode reward total was -79.000000. running mean: -53.523631\n",
            "ep 415.000000: resetting env. episode reward total was -79.000000. running mean: -54.797450\n",
            "ep 416.000000: resetting env. episode reward total was -68.000000. running mean: -55.457577\n",
            "ep 417.000000: resetting env. episode reward total was -68.000000. running mean: -56.084698\n",
            "ep 418.000000: resetting env. episode reward total was -68.000000. running mean: -56.680463\n",
            "ep 419.000000: resetting env. episode reward total was -90.000000. running mean: -58.346440\n",
            "ep 420.000000: resetting env. episode reward total was -90.000000. running mean: -59.929118\n",
            "ep 421.000000: resetting env. episode reward total was -68.000000. running mean: -60.332662\n",
            "ep 422.000000: resetting env. episode reward total was -79.000000. running mean: -61.266029\n",
            "ep 423.000000: resetting env. episode reward total was -68.000000. running mean: -61.602728\n",
            "ep 424.000000: resetting env. episode reward total was -68.000000. running mean: -61.922591\n",
            "ep 425.000000: resetting env. episode reward total was -90.000000. running mean: -63.326462\n",
            "ep 426.000000: resetting env. episode reward total was -90.000000. running mean: -64.660139\n",
            "ep 427.000000: resetting env. episode reward total was -68.000000. running mean: -64.827132\n",
            "ep 428.000000: resetting env. episode reward total was -79.000000. running mean: -65.535775\n",
            "ep 429.000000: resetting env. episode reward total was -68.000000. running mean: -65.658986\n",
            "ep 430.000000: resetting env. episode reward total was -68.000000. running mean: -65.776037\n",
            "ep 431.000000: resetting env. episode reward total was -68.000000. running mean: -65.887235\n",
            "ep 432.000000: resetting env. episode reward total was -68.000000. running mean: -65.992873\n",
            "ep 433.000000: resetting env. episode reward total was -90.000000. running mean: -67.193230\n",
            "ep 434.000000: resetting env. episode reward total was -68.000000. running mean: -67.233568\n",
            "ep 435.000000: resetting env. episode reward total was -68.000000. running mean: -67.271890\n",
            "ep 436.000000: resetting env. episode reward total was -68.000000. running mean: -67.308295\n",
            "ep 437.000000: resetting env. episode reward total was -68.000000. running mean: -67.342881\n",
            "ep 438.000000: resetting env. episode reward total was -79.000000. running mean: -67.925737\n",
            "ep 439.000000: resetting env. episode reward total was -68.000000. running mean: -67.929450\n",
            "ep 440.000000: resetting env. episode reward total was -79.000000. running mean: -68.482977\n",
            "ep 441.000000: resetting env. episode reward total was -90.000000. running mean: -69.558828\n",
            "ep 442.000000: resetting env. episode reward total was -68.000000. running mean: -69.480887\n",
            "ep 443.000000: resetting env. episode reward total was -68.000000. running mean: -69.406843\n",
            "ep 444.000000: resetting env. episode reward total was -68.000000. running mean: -69.336500\n",
            "ep 445.000000: resetting env. episode reward total was -79.000000. running mean: -69.819675\n",
            "ep 446.000000: resetting env. episode reward total was -68.000000. running mean: -69.728692\n",
            "ep 447.000000: resetting env. episode reward total was -79.000000. running mean: -70.192257\n",
            "ep 448.000000: resetting env. episode reward total was 944.000000. running mean: -19.482644\n",
            "ep 449.000000: resetting env. episode reward total was 944.000000. running mean: 28.691488\n",
            "ep 450.000000: resetting env. episode reward total was -68.000000. running mean: 23.856914\n",
            "ep 451.000000: resetting env. episode reward total was -68.000000. running mean: 19.264068\n",
            "ep 452.000000: resetting env. episode reward total was -79.000000. running mean: 14.350864\n",
            "ep 453.000000: resetting env. episode reward total was -79.000000. running mean: 9.683321\n",
            "ep 454.000000: resetting env. episode reward total was -79.000000. running mean: 5.249155\n",
            "ep 455.000000: resetting env. episode reward total was -68.000000. running mean: 1.586697\n",
            "ep 456.000000: resetting env. episode reward total was -68.000000. running mean: -1.892637\n",
            "ep 457.000000: resetting env. episode reward total was -68.000000. running mean: -5.198006\n",
            "ep 458.000000: resetting env. episode reward total was -68.000000. running mean: -8.338105\n",
            "ep 459.000000: resetting env. episode reward total was -68.000000. running mean: -11.321200\n",
            "ep 460.000000: resetting env. episode reward total was -68.000000. running mean: -14.155140\n",
            "ep 461.000000: resetting env. episode reward total was -79.000000. running mean: -17.397383\n",
            "ep 462.000000: resetting env. episode reward total was -68.000000. running mean: -19.927514\n",
            "ep 463.000000: resetting env. episode reward total was -79.000000. running mean: -22.881138\n",
            "ep 464.000000: resetting env. episode reward total was -68.000000. running mean: -25.137081\n",
            "ep 465.000000: resetting env. episode reward total was -68.000000. running mean: -27.280227\n",
            "ep 466.000000: resetting env. episode reward total was -68.000000. running mean: -29.316216\n",
            "ep 467.000000: resetting env. episode reward total was -68.000000. running mean: -31.250405\n",
            "ep 468.000000: resetting env. episode reward total was -68.000000. running mean: -33.087885\n",
            "ep 469.000000: resetting env. episode reward total was -79.000000. running mean: -35.383491\n",
            "ep 470.000000: resetting env. episode reward total was -68.000000. running mean: -37.014316\n",
            "ep 471.000000: resetting env. episode reward total was -79.000000. running mean: -39.113600\n",
            "ep 472.000000: resetting env. episode reward total was -90.000000. running mean: -41.657920\n",
            "ep 473.000000: resetting env. episode reward total was -79.000000. running mean: -43.525024\n",
            "ep 474.000000: resetting env. episode reward total was -68.000000. running mean: -44.748773\n",
            "ep 475.000000: resetting env. episode reward total was -79.000000. running mean: -46.461334\n",
            "ep 476.000000: resetting env. episode reward total was -68.000000. running mean: -47.538268\n",
            "ep 477.000000: resetting env. episode reward total was -68.000000. running mean: -48.561354\n",
            "ep 478.000000: resetting env. episode reward total was -68.000000. running mean: -49.533287\n",
            "ep 479.000000: resetting env. episode reward total was -79.000000. running mean: -51.006622\n",
            "ep 480.000000: resetting env. episode reward total was -68.000000. running mean: -51.856291\n",
            "ep 481.000000: resetting env. episode reward total was -68.000000. running mean: -52.663477\n",
            "ep 482.000000: resetting env. episode reward total was -68.000000. running mean: -53.430303\n",
            "ep 483.000000: resetting env. episode reward total was -68.000000. running mean: -54.158788\n",
            "ep 484.000000: resetting env. episode reward total was -68.000000. running mean: -54.850848\n",
            "ep 485.000000: resetting env. episode reward total was -90.000000. running mean: -56.608306\n",
            "ep 486.000000: resetting env. episode reward total was -68.000000. running mean: -57.177890\n"
          ]
        }
      ]
    }
  ]
}